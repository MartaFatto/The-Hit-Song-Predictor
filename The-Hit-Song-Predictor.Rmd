---
title: "THE HIT SONG PREDICTOR"
author: "Marta Fattorel & Fabio Taddei Dalla Torre"
---


```{r message=FALSE, warning=FALSE}
library(tree)
library(ISLR)
library(ggpubr)
library(ggplot2)
library(glmnet)
library(MASS)
library(randomForest)
library(e1071)
library(gbm)
library(caret)
library(dplyr)
library(reshape2)
library(scales)
library(pheatmap)
```


## DATA EXPLORATION


```{r}
dataset <- read.csv("dataset-of-10s.csv", header = TRUE)
# summary(dataset)
```

```{r}
# Add "featuring" variable
dataset$artist <- as.character(dataset$artist)
featuring <-  strsplit(dataset$artist,"Featuring")
dataset$featuring <- sapply(featuring, "[", 2 )
dataset$featuring <- as.factor(ifelse(is.na(dataset$featuring), 0, 1))
```

```{r}
# sapply(dataset, function(x) sum(is.na(x)))                       # no NA values
dataset <- dataset[!duplicated(dataset[, c('track', 'artist')]), ] # remove duplicates
options(scipen = 999)                                              # turn off scientific notation
dataset <- within(dataset, rm('uri'))                              # drop uri
dataset$target <- factor(dataset$target)                           # target into factor
```


Explore the variables distribution according to the success

```{r}
# create a new dataset with labels for target and featuring for better visualization
df.labels <- dataset[, 3:19]
df.labels$target <- ifelse(df.labels$target == 1, 'Hit', 'Flop')
df.labels$featuring <- ifelse(df.labels$featuring == 1, 'Yes', 'No')
df.labels$target <- factor(df.labels$target)
df.labels$featuring <- factor(df.labels$featuring)
```
```{r}
theme_set(theme_classic())
plot.songs <- function(x) {
  features <- names(x)
  for (i in seq_along(features)) {
 assign(paste("g", i, sep = ""),ggplot(x,aes_string(x = features[i])) + geom_density(aes(color = target)) + theme(legend.title = element_blank())) }
  g16 <- ggplot(df.labels, aes(x = featuring)) + geom_bar(aes(fill = target))
  print(ggarrange(g1, g2, g3, g4, g5, g6, g7, g8, g9, ncol = 3, nrow = 3, common.legend = T))
  print(ggarrange(g10, g11, g12, g13, g14, g15, g16, ncol = 3, nrow = 3, legend = FALSE))
  
}
plot.songs(df.labels)
```

Visualize with a heatmap the correlation among the numerical features of our dataset

```{r}
pheatmap(cor(as.matrix(dataset[, 3:17])), cell.width = 10, cell.height = 10) 
```



## STATISTICAL LEARNING MODELS

### LOGISTIC REGRESSION 

```{r}
glm.fit <- glm(target ~.,  family = binomial, data = dataset[, 3:19])
summary(glm.fit)
```

Perform cross-validation to estimate its performance (Validation Set Approach and 5-fold cross validation)

```{r}
set.seed(10)

train <- sample(nrow(dataset), nrow(dataset)/2)

full.lr <- glm(target ~.,  family = binomial, subset = train, data = dataset[, 3:19])
lr.prob <- predict(full.lr, newdata = dataset[-train,], type = 'response')
lr.pred <- rep(0, length(lr.prob))
lr.pred[lr.prob >= 0.5] <- 1                     
# table(lr.pred, dataset$target[-train]) confusion matrix        
full.err <- with(dataset, mean(lr.pred != target[-train]))

set.seed(10)

f <- 5
shuffled.data <- dataset[sample(nrow(dataset)),]
folds <- cut(seq(1, nrow(dataset)), breaks = f, labels=FALSE)
cv.errors <- rep(0, f)
for(i in 1:f){
  train <- which(folds != i, arr.ind = TRUE)
  full.lr <- glm(target ~., family = binomial, subset = train, data = shuffled.data[, 3:19])
  class <- ifelse(predict(full.lr, shuffled.data[-train,], type = 'response') >= 0.50, 1,0)
  # fold.error <- with(shuffled.data, mean(class != target[-train]))
  fold.error<- ifelse(class != shuffled.data[-train, ]$target, 1, 0)
  cv.errors[i] <- mean(fold.error)
}
cv.full <- mean(cv.errors)

cat("Full logistic regression test error with Validation Set is:", full.err, "\n")
cat("Full logistic regression test error with 5-fold CV is:", cv.full, "\n")
```


Compare the full logistic regression model with a regularization method in order to see which are the most relevant predictors. We choose **Lasso penalized logistic regression**

```{r}
set.seed(10)

x <- model.matrix(target ~., dataset[, 3:19])[, -1] # matrix of predictor variables
y <- dataset$target                                 # response variable

# Find the optimal lambda through cross-validation
cv.lasso <- cv.glmnet(x[train, ], y[train], alpha = 1, family = "binomial") 

lr.lasso <- glmnet(x[train, ], y[train], family = 'binomial', alpha = 1, lambda = cv.lasso$lambda.min)
lasso.prob <- predict(lr.lasso, newx = x[-train,], type = 'response') 
pred.classes <- ifelse(lasso.prob > 0.5, 1, 0)
lasso.err <- mean(pred.classes != y[-train])
cat("Lasso logistic regression test error is:", lasso.err, "\n")

log.lasso <- glmnet(x, y, family = 'binomial', alpha = 1)
predict(log.lasso, type = 'coefficients', s = cv.lasso$lambda.min)[1:17, ]

```
No feature selection since no coefficients are exactly 0


### SUPPORT VECTOR MACHINES

Before fitting the model, in order to improve the final prediction, we can tune the hyperparameters `cost` and `gamma`. 
We tried with `cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)` and `gamma=c(0.01, 0.1, 1, 4)`. 
However, here we consider a limited range of values since the process is computationally expensive.

```{r}
set.seed(10)

train <- sample(nrow(dataset), nrow(dataset)/2)
dataset$target <- as.factor(dataset$target)

tune.out <- tune(svm, target ~., data = dataset[train, 3:19], kernel = "radial", ranges = list(cost = c(1, 5), gamma = c(0.1, 1)))
# summary(tune.out)
```

The optimal cost results to be 1, while the best gamma is 0.1. We proceed by fitting our model on the train data to predict the test error rate

```{r}
best.svm <- tune.out$best.model
opt.gamma <- best.svm$gamma
opt.cost <- best.svm$cost

best.pred <- predict(best.svm, newdata = dataset[-train, 3:19])
# table(best.pred, dataset$target[-train]) confusion matrix
svm.err <- with(dataset, mean(best.pred != target[-train]))
cat("Support vector machines test error is:", svm.err, "\n")
```


### DECISION TREES

```{r}
set.seed(1234)

# use the dataset with labels created at the beginning for better trees visualization

train <- sample(nrow(df.labels), nrow(df.labels)/2)
tree.train <- tree(target ~., subset = train, data = df.labels) 
tree.pred <- predict(tree.train, df.labels[-train,], type = "class") 

tree.err <- mean(tree.pred != df.labels$target[-train])
cat("Classification tree test error is:", tree.err, "\n")
```

Prune the tree

```{r}
cv.song <- cv.tree(object = tree.train, FUN = prune.misclass)
cv.song
```

```{r}
opt.size <- cv.song$size[which.min(cv.song$dev)]
prune.songs <- prune.misclass(tree.train, best = opt.size)
```

```{r}
par(mfrow = c(1, 2))
plot(tree.train)
text(tree.train, pretty = 0, cex = .7)
title("Classification tree")
plot(prune.songs)
text(prune.songs, pretty = 0, cex = .7)
title("Pruned tree")
```

In this case we can see that the pruned tree and the initial one are the same


**Performing Boosting**

The aim of boosting is improving the result of the model by reducing tree variance

```{r}
dataset$target <- as.numeric(dataset$target)
dataset$target <- ifelse(dataset$target == 1, 0, 1) # in order to run the method we have to change the variable target into numeric
```

Before fitting the model, tune the hyperparameters *shrinkage*, *tree depth* and *number of trees*. 
Since this process is computationally expensive we have tried it with a wider range of parameters (`learning_rate = c(0.3, 0.1, 0.05, 0.01, 0.005)` and `interaction.depth = c(3, 5, 7, 9)`), but here we consider only some of those for efficiency and we perform 5-fold cross validation to choose the best ones.

```{r}
# create grid  with all the values of the hyperparameters 
hyper_grid <- expand.grid(
  learning_rate = c(0.1, 0.05),
  interaction.depth = c(5, 7),
  error = NA,
  trees = NA
)

# fit the model with different parameters 
for(i in seq_len(nrow(hyper_grid))) {

  set.seed(10) # for reproducibility
    m <- gbm(
      formula = target ~ ., 
      data = dataset[train, 3:19],
      distribution = "bernoulli",
      n.trees = 300,
      shrinkage = hyper_grid$learning_rate[i], 
      interaction.depth = hyper_grid$interaction.depth[i], 
      cv.folds = 5
  )
  
  hyper_grid$error[i]  <- min(m$cv.error)
  hyper_grid$trees[i] <- which.min(m$cv.error) 
}

# results
hyper <- arrange(hyper_grid, error)
best.shrinkage <- arrange(hyper, error)[1, 1]
best.depth <- arrange(hyper, error)[1, 2]
best.tree <- arrange(hyper, error)[1, 4] # best number of trees to grow

hyper[1, ]
```

Now we can fit the model with these hyperparameters and predict the test error rate

```{r echo=TRUE, include=FALSE}
boost.train <- gbm(target ~ ., data = dataset[train, 3:19], distribution = "bernoulli", n.trees = best.tree, interaction.depth = best.depth, shrinkage = best.shrinkage, verbose = F)

boost.test <- predict(boost.train, newdata = dataset[-train, 3:19], n.trees = best.tree, type="response")
# table(boost.test, df_song$target[-train])
boost.test <- ifelse(boost.test <= 0.5, 0 ,1)
test.boost <- mean(boost.test != dataset$target[-train]) # test error

data <- summary(boost.train)[2]
```

```{r}
cat("Boosted tree test error is:", test.boost, "\n")

data$rel.inf <- round(data$rel.inf,2)
data["Variables"] <- factor(row.names(data), levels = row.names(data))

ggplot(data, aes(x = rel.inf, y = Variables, label = rel.inf)) +
  geom_col(position = 'dodge', fill="royalblue3", width = .5) +
  geom_text(position = position_stack(vjust = 1.05),
            vjust = -0.5,
            size = 3) +
  scale_y_discrete(limits = rev(levels(data$Variables))) +
  coord_cartesian(xlim = c(0, 100)) +
  labs(title="Predictors relative influence calculated by boosting",
       x = "Relative influence",
       y= NULL) +
  theme_minimal()+
  theme(panel.grid.major.y = element_blank())
```

```{r}
dataset$target <- as.factor(dataset$target) # changing back target to factor
```


**Performing Random Forests**

Random Forests algorithms prevents correlation among trees.
Before fitting the model, we tune the hyperparameters `mtry` and `ntree` which refers to the number of trees

```{r}
set.seed(10)

# Tune mtry with 5-fold cross validation
trControl <- trainControl(method = "cv", number = 5, search = "grid")

tuneGrid <- expand.grid(.mtry = c(1:8))
rf <- train(target ~ ., data = dataset[train, 3:19], method = "rf", metric = "Accuracy", tuneGrid = tuneGrid, importance = T, ntree = 100, trControl = trControl)

opt.mtry <- rf$bestTune$mtry
```

To tune `ntree` we tried with `c(250, 300, 500, 800, 1000)`. Here, to reduce the complexity we give the algorithm just two of these values
```{r}
set.seed(10)

# Tune ntrees with 5-fold cross validation
trControl <- trainControl(method = "cv", number = 5, search = "grid")
store_maxtrees <- list()


tuneGrid <- expand.grid(.mtry = opt.mtry) 
for (ntree in c(500, 800)){
  rf_maxtrees <- train(target ~ ., data = dataset[train, 3:19], 
              method = "rf", metric = "Accuracy", 
              tuneGrid = tuneGrid, importance = T, 
              ntree = ntree, trControl = trControl)
  key <- toString(ntree)
  store_maxtrees[[key]] <- rf_maxtrees
}

results_tree <- resamples(store_maxtrees) # the best number of trees is 800
# summary(results_tree) 
```

In this case we do not need to compute the test error with CV, instead we can look at the OOB classification error to estimate it

```{r}
rf.train <- randomForest(target ~ ., data = dataset[, 3:19], subset = train, mtry = opt.mtry, importance = TRUE, ntree = 800) 

rf.OOB <- rf.train$err.rate[nrow(rf.train$err.rate), 1]
cat("Random Forests test error is:", rf.OOB, "\n")

varImpPlot(rf.train, main = "Random Forests - Variable importance")
rf.train
```

## RESULTS 

```{r}
model <- c("Simple Tree","Lasso log. reg.", "Full log. reg.", "SVM", "Random Forests", "Boosted Tree")
test.error <- c(tree.err, lasso.err, cv.full, svm.err,  rf.OOB, test.boost)
test.error <- round(test.error, digits = 3)
performances <- data.frame(model, test.error)
performances$model <- factor(performances$model, levels = performances$model)

theme_set(theme_classic())
ggplot(performances, aes(x=model, y=test.error, label = scales::percent(test.error))) +
  geom_col(position = 'dodge', fill="royalblue3", width = .5) + 
  geom_text(position = position_dodge(width = .9),    
            vjust = -0.5,
            size = 3) +
  scale_y_continuous(labels = function(x) paste0(x * 100, '%')) +
  coord_cartesian(ylim = c(0,1)) +
  labs(title="Test error according to each algorithm", 
       y=NULL,
       x=NULL) +
  theme_minimal()+
  theme(panel.grid.major.x = element_blank())
```

